---
title: 配置中心ZooKeeper,Consul,Apollo,Nacos
---

[toc]

### 前言

> 这里讲的配置中心范畴会宽一点,有可能是这样的,包含了 服务注册中心+配置中心.

1. 在整体业务越来越复杂,云计算和大数据快速发展的今天,我们部署的服务越来越多,前后端隔离之后双方也在自己的道路上越走越远.特别是微服务的出现,当我们对服务进行拆分后就回出现一个新的问题,服务与服务,前端与后端之间应该怎么调用?

大部分的服务都是使用 IP:PORT 的方式来提供服务的,在只有少量的服务的时候,客户端直连应该是效率最高的,但是随时业务和架构的复杂之后,我们就很难全部采用直连的方式来调用了.

所以第一个想要解决的问题就是 服务端和客户端的之间发现调用关系的解耦,这就是服务注册中心解决的问题.

服务端将自己的服务信息可以之策到注册中心,客户端可以从注册中心发现所依赖的服务,然后再去请求指定的服务.

2. 至于配置中心的需求也很好理解,传统的静态配置方式要想修改某个配置只能修改之后重新发布应用，要实现动态性，可以选择使用数据库，通过定时轮询访问数据库来感知配置的变化。轮询频率低感知配置变化的延时就长，轮询频率高，感知配置变化的延时就短，但比较损耗性能，需要在实时性和性能之间做折中。

配置中心专门针对这个业务场景，兼顾实时性和一致性来管理动态配置。

3. 讲到这里都是从现状尝试解决问题,但实现了服务注册中心和配置中心,再它们结合起来后可以做到更多:

- 监控:服务实例会处于动态的变化中,我们有需要监控服务实例这个粒度简况状况的需求
- 负载均衡:更加动态 实时 可以配置的路由规则可以更灵活的实现服务的负载均衡,当然规则也会更复杂.
- 最后就是配置中心的一些需求了,配置的动态发布,修改及其权限控制,灰度发布,版本管理和回滚,等等都是传统的静态文件所无法做到的

### CAP原则

这里不得不提到的就是分布式中的一个重要理论 CAP原则.即在一个分布式系统中，Consistency(一致性)、Availability(可用性)、Partition Tolerance(分区容错性)，不能同时成立。

*   一致性：它要求在同一时刻点，分布式系统中的所有数据备份都处于同一状态。
*   可用性：在系统集群的一部分节点宕机后，系统依然能够响应用户的请求。
*   分区容错性：在网络区间通信出现失败，系统能够容忍。

一般来讲，基于网络的不稳定性，分布容错是不可避免的，所以我们默认CAP中的P总是成立的。一致性的强制数据统一要求，必然会导致在更新数据时部分节点处于被锁定状态，此时不可对外提供服务，影响了服务的可用性，反之亦然。因此一致性和可用性不能同时满足。在注册中心的发展上面，一直有两个分支：一个就是 CP 系统，追求数据的强一致性。还有一个是 AP 系统，追求高可用与最终一致。

在不同的业务场景下会有不同的选择.就我个人而言,我觉得作为服务注册中心和配置中心,AP才是正确的方向,由于强一致性必定导致异常情况下会有一段时间用于恢复一致状态而导致无法对外服务,这时整个业务集群就表现为宕机了.

> 这里认为 ZooKeeper 和 Consul 都是 CP的,而 Apollo 使用了Eureka 作为存储,实现是 AP 的,Nacos则相对的结合了 CP和AP.

### 服务注册中心解决方案

设计或者选型一个服务注册中心，首先要考虑的就是服务注册与发现机制。纵观当下各种主流的服务注册中心解决方案，大致可归为三类：

- 应用内：直接集成到应用中，依赖于应用自身完成服务的注册与发现，最典型的是Netflix提供的Eureka
- 应用外：把应用当成黑盒，通过应用外的某种机制将服务注册到注册中心，最小化对应用的侵入性，比如Airbnb的SmartStack，HashiCorp的Consul
- DNS：将服务注册为DNS的SRV记录，严格来说，是一种特殊的应用外注册方式，SkyDNS是其中的代表

> 注1：对于第一类注册方式，除了Apollo(Eureka)这种一站式解决方案，还可以基于ZooKeeper或者Etcd自行实现一套服务注册机制，这在大公司比较常见，但对于小公司而言显然性价比太低。
> 注2：由于DNS固有的缓存缺陷，本文不对第三类注册方式作深入探讨。

除了基本的服务注册与发现机制，从开发和运维角度，至少还要考虑如下五个方面：

- 测活：服务注册之后，如何对服务进行测活以保证服务的可用性？
- 负载均衡：当存在多个服务提供者时，如何均衡各个提供者的负载？
- 集成：在服务提供端或者调用端，如何集成注册中心？
- 运行时依赖：引入注册中心之后，对应用的运行时环境有何影响？
- 可用性：如何保证注册中心本身的可用性，特别是消除单点故障？

> 不得不说由于java和.net环境的不同,这些主流方案都是java实现的或者java天然集成的,.net好苦
> 所以我还会多考虑一点,在.net环境中集成使用的过度曲线和学习成本

| - | Nacos | Eureka | Consul | Zookeeper |
| --- | --- | --- | --- | --- |
| 一致性协议 | CP+AP | AP | CP | CP |
| 健康检查 | TCP/HTTP/MYSQL/Client Beat | Client Beat | TCP/HTTP/gRPC/Cmd | Keep Alive |
| 负载均衡策略 | 权重/metadata/Selector | Ribbon | Fabio | - |
| 雪崩保护 | 有 | 有 | 无 | 无 | 无 |
| 自动注销实例 | 支持 | 支持 | 不支持 | 不支持 | 支持 |
| 访问协议 | HTTP/DNS | HTTP | HTTP/DNS | TCP |
| 监听支持 | 支持 | 支持 | 支持 | 支持 |
| 多数据中心 | 支持 | 支持 | 支持 | 不支持 |
| 跨注册中心同步 | 支持 | 不支持 | 支持 | 不支持 |

#### 核心概念

服务 (应用)(分组) :
这是系统中的基本单位,用于注册信息和配置的相互隔离

环境 (命名空间) :
这是由配置或服务所使用的场景决定的,比如一般有 开发环境,开发测试环境,用户测试环境,生产环境

集群 : 
服务在不同的环境中可以搭建不同的集群,也可以起到物理隔离的作用,如果只有一个集群,一般默认为default集群

#### 主要功能

灰度发布 :
配置的灰度发布是配置中心比较重要的功能，当配置的变更影响比较大的时候，需要先在部分应用实例中验证配置的变更是否符合预期，然后再推送到所有应用实例。

权限管理 : 
配置的变更和代码变更都是对应用运行逻辑的改变，重要的配置变更常常会带来核弹的效果，对于配置变更的权限管控和审计能力同样是配置中心重要的功能。

版本管理&回滚 :
当配置变更不符合预期的时候，需要根据配置的发布版本进行回滚。

配置格式校验 :
应用的配置数据存储在配置中心一般都会以一种配置格式存储，比如Properties、Json、Yaml等，如果配置格式错误，会导致客户端解析配置失败引起生产故障，配置中心对配置的格式校验能够有效防止人为错误操作的发生，是配置中心核心功能中的刚需。

监听查询 :
当排查问题或者进行统计的时候，需要知道一个配置被哪些应用实例使用到，以及一个实例使用到了哪些配置。

多环境 :
在实际生产中，配置中心常常需要涉及多环境或者多集群，业务在开发的时候可以将开发环境和生产环境分开，或者根据不同的业务线存在多个生产环境。

多集群 :
当对稳定性要求比较高，不允许各个环境相互影响的时候，需要将多个环境通过多集群的方式进行物理隔离。

配置实时推送 :
当配置变更的时候，配置中心需要将配置实时推送到应用客户端。

高可用集群部署 :
配置中心和注册中心的稳定性要求较高,单机或单实例部署时不可取的,所以集群部署也属于一个必备的需求.

多语言支持 :
一个公司的各个系统可能语言不尽相同, 最好是公司主要的开发语言有现成的sdk支持.不然就需要有open api自行实现了.

### 主要框架

#### ZooKeeper

ZooKeeper本身只是实现了一个原子的变更,通过watcher可以方便的监视路径和节点数据变更的功能,通过这个功能来实现服务注册中心的话需要服务自行操作节点来进行服务注册.客户端也需要实现对节点数据的读取.

*   ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器。
*   Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和  Observer 都只能提供读服务
*   Server：Server中存在两种类型：Follower和Observer。其中Follower接受客户端的请求并返回结果(事务请求将转发给Leader处理)，并在选举过程中参与投票；Observer与Follower功能一致，唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。
*   Client：请求发起方，Server和Client之间可以通过长连接的方式进行交互。如发起注册或者请求集群信息等。
*   集群间通过 Zab 协议（Zookeeper Atomic Broadcast）来保持数据的一致性。

使用 Fast Paxos 或 Basic Paxos 作为一致性算法

使用Zab (ZooKeeper Atomic Broardcast)协议作为支持崩溃恢复的原子广播协议

使用ZooKeeper作为服务中心完全看自己对其的开发和封装程度,但是也仅仅能作为服务注册和服务发现,写入配置,读取配置相对简单的功能了

为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。

客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 长连接来使用服务。客户端使用这个 TCP 链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。

ZooKeeper在设计时就谨遵CP原则,任何时候对ZooKeeper的访问请求都能得到一致的数据结果,但不能保证每次访问都能得到返回.在大多数分布式环境中，尤其是涉及到数据存储的场景，数据一致性应该是首先被保证的，这也是 Zookeeper 设计紧遵CP原则的另一个原因。但是对于服务发现来说，情况就不太一样了，针对同一个服务，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。一般具体的服务节点也会对请求做一致性hash保证业务的最终一致性确定.

对于服务消费者来说，能消费才是最重要的，消费者虽然拿到可能不正确的服务实例信息后尝试消费一下，也要胜过因为无法获取实例信息而不去消费.打个比方来说就是,618下单的时候折扣券没读出来,有可能消费者下单了会要求保价,这肯定要比由于找不到这个服务导致业务挂掉下不了单用户卡住了流程好的多.

当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30~120s，而且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。

在云部署环境下， 因为网络问题使得zk集群失去master节点是大概率事件，虽然服务能最终恢复，但是漫长的选举事件导致注册长期不可用是不能容忍的。

#### Consul

Consul是HashiCorp公司推出的开源工具，基于Go语言开发的支持多数据中心分布式高可用的服务发布和注册服务软件，采用Raft算法保证服务的一致性，且支持健康检查。因为通过Golang实现，因此具有天然可移植性(支持Linux、windows和Mac OS X)；安装包仅包含一个可执行文件，方便部署，与Docker等轻量级容器可无缝配合。Consul采用主从模式的设计，使得集群的数量可以大规模扩展，集群间通过RPC的方式调用(HTTP和DNS)。

*   Client：作为一个代理(非微服务实例)，它将转发所有的RPC请求到Server中。作为相对无状态的服务，它不持有任何注册信息。
*   Server：作为一个具备扩展功能的代理，它将响应RPC查询、参与Raft选举、维护集群状态和转发查询给Leader等。
*   Leader-Server：一个数据中心的所有Server都作为Raft节点集合的一部分。其中Leader将负责所有的查询和事务(如服务注册)，同时这些事务也会被复制到所有其他的节点。
*   Data Center：数据中心作为一个私有的，低延迟和高带宽的一个网络环境。每个数据中心会存在Consul集群，一般建议Server是3-5台(考虑到Raft算法在可用性和性能上取舍)，而Leader只能唯一，Client的数量没有限制，可以轻松扩展。Consul 通过 WAN 的 Gossip 协议，完成跨数据中心的同步；而且其他的产品则需要额外的开发工作来实现。

Consul也是CP的,特别时其在崩溃之后的恢复由于一致性协议的延迟问题,导致会中断外界的访问.

使用Raft算法作为一致性算法,Raft算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制。

使用Gossip协议 (Epidemic Protocol 流行病协议 流言算法) 作为最终一致性协议.

![](https://raw.githubusercontent.com/OliverRen/olili_blog_img/master/配置中心ZooKeeper,Consul,Apollo,Nacos/2020811/1597125034587.png)

**Gossip 的特点（优势）**
1. 扩展性
网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致。
2. 容错
网络中任何节点的宕机和重启都不会影响 Gossip 消息的传播，Gossip 协议具有天然的分布式系统容错特性。
3. 去中心化
Gossip 协议不要求任何中心节点，所有节点都可以是对等的，任何一个节点无需知道整个网络状况，只要网络是连通的，任意一个节点就可以把消息散播到全网。
4. 一致性收敛
Gossip 协议中的消息会以一传十、十传百一样的指数级速度在网络中快速传播，因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了 logN。
5. 简单
Gossip 协议的过程极其简单，实现起来几乎没有太多复杂性。

**Gossip 的缺陷**
分布式网络中，没有一种完美的解决方案，Gossip 协议跟其他协议一样，也有一些不可避免的缺陷，主要是两个：
1. 消息的延迟
由于 Gossip 协议中，节点只会随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网的，因此使用 Gossip 协议会造成不可避免的消息延迟。不适合用在对实时性要求较高的场景下。
2. 消息冗余
Gossip 协议规定，节点会定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤，因此就不可避免的存在消息重复发送给同一节点的情况，造成了消息的冗余，同时也增加了收到消息的节点的处理压力。而且，由于是定期发送，因此，即使收到了消息的节点还会反复收到重复消息，加重了消息的冗余。

#### Apollo (Eureka注册中心)

[https://github.com/ctripcorp/apollo](https://github.com/ctripcorp/apollo)

Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。

Apollo可以运行多个实例来构建集群,但不同于ZooKeeper的选举leader的过程,Apollo采用的是P2P的对的对等通信,这事一种去中心化的架构.这种架构中,节点通过彼此相互注册来提高可用性.当一个节点接收到客户端请求后,会使用 `replicate To Peer`操作,将请求复制到集群中他所知的其他节点中

Apollo是AP的,在集群中,只要有一台节点还在,就保证整个服务是可用的,只不过查到的信息有可能不是最新的,不能保证强一致性.
同时在Apollo中注册的服务信息还会被缓存在请求的客户端中.
除此之外,Apollo还有一些保护机制,当注册中心出现一些极端的网络问题时,不会过度的清理数据,同时依然可以记录接受到的请求,当网络稳定后,可以更快的从崩溃中恢复,并将新注册的信息同步到其他节点.

作为服务的注册中心,Apollo提供的sdk来给服务端使用,但相对的这对原应用有一定程度的破坏性,不过考虑到我们原来使用zookeeper的方式也有一个步骤,所以也不用太过于担忧.

Apollo相对来说对.net环境的sdk,文档较多,可以很方便的一步一步开始.

作为配置中心,基本所有的功能都有提供,不过从其他使用的案例来讲,由于整体分离成了几部分,部署和配置较为复杂.不过随着docker封装和打包脚本,应该也都得以了简化

#### Nacos

Nacos是阿里巴巴开源的服务注册配置中心.从其生态图可见,其与java环境和运环境的联系很紧密,也有python,nodejs和go的sdk.但是对.net环境就不是那么的友好了.不过也提供了OpenAPI可以方便的自己进行封装.

从整体功能上来说, ZooKeeper和Consul只能说是遵循CP的上一代实现,在某些场景依然能很好的工作,不过随着服务更加碎片化,业务更加复杂,已经很难应付现在对异常友好处理的原则了.

至于Apollo和Nacos,虽然开源时间有长短,核心概念有些侧重有些弱化,但就整体功能来说都是完备的.

由于我对Apollo也仅仅是单机简单试用,对Nacos只是从文档上进行了学习,对于技术选型来说肯定是不够的,下面就以他们两个作为对比来进行一下说明,至于选用哪种,个人觉得都是可用的,但在不同的团队和不同的个人喜好上可以区分,比如我就因为.net环境的友好更倾向于Apollo,也许你们公司大部分选用的开发语言是java,那么阿里出品的Nacos肯定会更加得心应手.

#### Apollo vs Nacos

1. 应用
Apollo的配置主要都是在应用下的
Nacos应用概念较弱,主要都是以服务来进行配置,可以使用Group来作为应用

2. 环境
Apollo使用环境,Nacos使用命名空间,其实是一个意思,可以作为环境逻辑隔离

3. 集群
Apollo和Nacos都支持在不同环境下部署多个集群

4. 灰度发布
Apollo可以在portal中灰度发布指定的机器,再全量发布
Nacos 在1.1.0之后也支持了灰度发布

5. 权限管理
Apollo通过项目的维度来进行权限管理,项目owner可以分配权限.
Nacos还在开发中

6. 版本管理&回滚
Apollo和Nacos都具备配置的版本管理和回滚能力

7. 配置格式校验
Apollo和Nacos都会对配置格式的正确性进行检验，可以有效防止人为错误。

8. 监听查询
当排查问题或者进行统计的时候，需要知道一个配置被哪些应用实例使用到，以及一个实例使用到了哪些配置。

9. 配置实时推送
Nacos和Apollo配置推送都是基于HTTP长轮询，客户端和配置中心建立HTTP长联接，当配置变更的的时候，配置中心把配置推送到客户端。